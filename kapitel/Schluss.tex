
\section{Schluss}
\label{sec:conclusion}
In dieser Arbeit haben wir zwei erweiterte Ansätze des \emph{Reinforcemnt Learning} vergleichend gegenübergestellt. Wir konnten zeigen, dass beide Konzepte, trotz ihrer theoretischen Differenzen, in gewissen Bereichen eine durchaus nicht unähnliche Funktionsweise aufweisen. Insbesondere findet sowohl das \emph{Curiosity}-basierte Modell, als auch der \emph{Diversity}-Ansatz eine adäquate Lösung für das in Kapitel \ref{sec:sparse_reward} vorgestellte \emph{Sparse Reward Problem}.


Wir sehen in beiden Ansätzen großes Potential. Vor allem das noch nicht häufig thematisierte Vorgehen des Erlernens von Fähigkeiten mittels Diversity sollte weiter erforscht werden.