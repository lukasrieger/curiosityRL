
\section{Verwandte Arbeiten}
\label{sec:related}

Der Ansatz der Curiosity kommt überwiegend im Bereich des Reinforcement-Learning zum Einsatz. Das Konzept findet hier vielfältige Anwendung in Problemdomänen, in denen traditionelle Algorithmen aufgrund der spärlichen externen Belohnung nur mit einigen Schwierigkeiten agieren können. Beispielsweise nutzen \citeauthor{8683033} in \cite{8683033} den Curiosity-Ansatz zur Dialoggenerierung, um mehr über den Nutzer zu erfahren. Ein ähnlicher Ansatz kommt in \cite{luo2019curiositydriven} zum Einsatz. Ziel des Reinforcement Algorithmus ist es hier, aussagekräftige Beschreibungen von Bildern aus möglichst unterschiedlichen Perspektiven zu generieren. Der Curiosity-Ansatz hilft laut \citeauthor{luo2019curiositydriven} dabei, akkurate und zugleich unterschiedliche Texte zu einem gegebenen Bild zu erzeugen. 

Es ist allerdings anzumerken, dass beide Ansätze ihren \emph{Curisoty-Reward} auf Basis des \emph{Prediction-Error} generieren. Ein solcher Ansatz wird von Schmidhuber auch in \cite{curiosity_schmidhuber} beschrieben, allerdings kritisiert Schmidhuber diesen aufgrund einiger Mängel. Anders als der in Kapitel \ref{sec:KompressorProgress} beschriebene, auf dem Kompressor-Fortschritt basierende Ansatz, läuft der \emph{Prediction-Error} basierte Ansatz Gefahr, stetig solche Aktionen auszuwählen, die zwar zu neuen, noch nicht gänzlich komprimierten Zuständen führen, welche allerdings nicht zwingend \emph{interessant} sein müssen. \cite{curiosity_schmidhuber}

\smallspace

Das Thema Diversity taucht nicht häufig in Zusammenhang mit RL Problemen auf. Jedoch nutzen \citeauthor{hong2018diversity} Diversity in \cite{hong2018diversity} als Erkundungsstrategie für \textit{Deep Reinforcement Learning}. Hierbei soll die modifizierte \textit{Loss Function} den Agenten dazu anregen, sich von bisherigen Policies zu differenzieren, während er trotzdem optimal handelt. So wird erreicht, dass der Agent seine Umgebung effizient erkundet \cite{hong2018diversity}.

Viel eher wird Diversity allerdings nach \cite{gabor2018inheritance} innerhalb von Literatur, die sich mit \textit{Evolutionären Algorithmen} beschäftigen, behandelt. Die Anwendung auf diesem Gebiet ist wesentlich naheliegender, da hier eine vielfältige Population gewünscht ist. Diversity soll verhindern, dass lediglich ein lokales Optimum gefunden wird \cite{gabor2018inheritance}. Hier lässt sich ein gewisser Zusammenhang zu \cite{diversity_eysenbach} erkennen, da Diversity in beiden Fällen dazu eingesetzt wird, möglichst vielfältige Entitäten zu generieren. In \cite{diversity_eysenbach} handelt es sich hierbei um die gelernten Fähigkeiten, bei Evolutionären Algorithmen sind es die Individuen einer Population, welche die Lösungskandidaten repräsentieren.